{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "fisHU-QDZtDW",
        "EQPw09QYZ2Vj",
        "gpzLf-rlZ7MA"
      ],
      "authorship_tag": "ABX9TyPWngNagY2eIPipLH83JXml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AanchalA/MakeMore-with-Andrej-Karpathy/blob/main/NanoGPT_on_TinyShakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "0Svu0g4cy1_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameters"
      ],
      "metadata": {
        "id": "7cjT1WHNy4e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameters\n",
        "\n",
        "BLOCK_SIZE = 8                      ## Context Length: how many characters do we take to predict the next one?\n",
        "BATCH_SIZE = 32\n",
        "MAX_ITERS = 3000\n",
        "EVAL_ITERS = 200\n",
        "EVAL_INTERVAL = 300\n",
        "LEARNING_RATE = 1e-3                ## Actual Models usually lr=3e-4\n",
        "NUM_EMBEDDING_DIMS = 32             ## Number of embedding dimensions\n",
        "# NUM_HIDDEN_UNITS = 128            ## Number of Neurons in the Hidden Layer of the MLP\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# g = torch.Generator().manual_seed(2147483647)                   # for reproducibility\n",
        "torch.manual_seed(1337);          ## seed rng for reproducibility"
      ],
      "metadata": {
        "id": "CyVNOeXmy5r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the DataSet"
      ],
      "metadata": {
        "id": "oLVSLOHNy9La"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lcZ44gVytjd",
        "outputId": "38d7b420-b60b-445c-a38b-82d65d86a384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-21 05:11:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-21 05:11:38 (54.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Length of dataset in characters: {len(text)} Characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0bUDyE7y_Sp",
        "outputId": "1cb62a6f-1745-4942-b6f4-9240888d61cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 1115394 Characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[365:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c0gVhPyzBNn",
        "outputId": "0a91b360-7bd6-4bcb-8205-94bbbee133a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "print(f\"List of all unique characters: {''.join(chars)}\")\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(f\"\\n{vocab_size=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdfp3r_EzY3m",
        "outputId": "df2cf4fa-15dd-4c0a-8804-6fa9b1387b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of all unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "vocab_size=65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "-1wM22rBzlO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder and Decoder"
      ],
      "metadata": {
        "id": "-LYcNg-u0sgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[ind] for ind in l])"
      ],
      "metadata": {
        "id": "pfWrXP9o0sZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlhXZNg1gze",
        "outputId": "dec0a185-d39d-46bf-cd65-c9a30f90bbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the text"
      ],
      "metadata": {
        "id": "xxm8QrdN2nbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)             ## torch.long = int\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iJujJfp1kMU",
        "outputId": "c90913f6-6c89-4264-8d63-a7b56a4e5455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train - Validation Split"
      ],
      "metadata": {
        "id": "wmh0ZwXT2sRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "\n",
        "n = int(0.9 * len(data))            # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "YHcwD2SA2xOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Data Size: {len(train_data)}\")\n",
        "print(f\"Validation Data Size: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV8x0-Lj3HGl",
        "outputId": "98291174-d0c2-44ab-c7d5-b3ca93b77f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Size: 1003854\n",
            "Validation Data Size: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Batching"
      ],
      "metadata": {
        "id": "VT6RZttT4H4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE              # BLOCK_SIZE - what is the maximum context length for predictions?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pm6BxIv9JXB",
        "outputId": "f7e2189b-a5e2-4593-a3ca-b49442f7c80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[: BLOCK_SIZE + 1]                ## Each \"block_size\" will have \"block_size\" number of individual training examples, \"block_size+1\" will be the target for the entire context in bloak_size"
      ],
      "metadata": {
        "id": "nN566rD44eow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4601a896-ab77-4cc7-85e2-a27b5b18cbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[: BLOCK_SIZE]\n",
        "y = train_data[1 : BLOCK_SIZE + 1]\n",
        "\n",
        "for time_step in range(BLOCK_SIZE):\n",
        "    context = x[: time_step + 1]\n",
        "    target = y[time_step]\n",
        "    print(f\"When input is {context}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdoXNC7m9Vqa",
        "outputId": "afae5c21-fc08-4e6f-90ae-f87c3685fccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]), the target is: 47\n",
            "When input is tensor([18, 47]), the target is: 56\n",
            "When input is tensor([18, 47, 56]), the target is: 57\n",
            "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE              # BATCH_SIZE - how many independent sequences will we process in parallel?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rztZiBgqAq9C",
        "outputId": "92067817-6358-4ba5-f97d-87360b4b5c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))              ## Picking BATCH_SIZE number of random offsets into the dataset.\n",
        "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in ix])\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "nUwTPgrTAtTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('inputs:\\n')\n",
        "print(f\"SHAPE: {xb.shape}\")\n",
        "# print(f\"DATA: {xb}\")\n",
        "\n",
        "print('\\n', '-'*45, '\\n')\n",
        "\n",
        "print('targets:\\n')\n",
        "print(f\"SHAPE: {yb.shape}\")\n",
        "# print(f\"DATA: {yb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1VdkuJCCE7F",
        "outputId": "66151fb4-9ce2-4e7c-b83b-41fb1cde1ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "\n",
            "SHAPE: torch.Size([32, 8])\n",
            "\n",
            " --------------------------------------------- \n",
            "\n",
            "targets:\n",
            "\n",
            "SHAPE: torch.Size([32, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in range(BATCH_SIZE):                 ## Batch Dimension\n",
        "    for time_step in range(BLOCK_SIZE):             ## Time Dimension - for each time_step in the context window.\n",
        "        context = xb[batch, : time_step + 1]\n",
        "        target = yb[batch, time_step]\n",
        "        # print(f\"When the context is {context.tolist()}, the target is: {target}\")\n",
        "# There will be a total of BATCH_SIZE * BLOCK_SIZE number of examples."
      ],
      "metadata": {
        "id": "FBxedeTCEBe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Out input to the transformer\n",
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZicL3_FbGa",
        "outputId": "3850524f-4e21-4ee0-901f-436e1327c5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
            "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
            "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
            "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
            "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
            "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
            "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
            "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
            "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
            "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
            "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
            "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
            "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
            "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
            "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
            "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
            "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
            "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
            "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
            "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
            "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
            "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
            "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
            "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
            "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
            "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
            "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
            "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
            "        [53, 58,  6,  1, 51, 63,  1, 50]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation Function"
      ],
      "metadata": {
        "id": "T639_XKEspn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(EVAL_ITERS)\n",
        "        for k in range(EVAL_ITERS):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "5SqRssY0spBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-Gram Model"
      ],
      "metadata": {
        "id": "JwhvzDBvFakF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch 2.0\n",
        "# model = torch.compile(model)                    ## Model acceleration- Needs A100???\n",
        "# Also look at acclerate lib - they have released a few updated that you can use."
      ],
      "metadata": {
        "id": "TkB4Ob_tF0L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits for the next token from the look-up table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, NUM_EMBEDDING_DIMS)               ## As the model learns, these embedding weights get updated.\n",
        "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, NUM_EMBEDDING_DIMS)            ## Each token in the context (from 0 to BLOCK_SIZE - 1) will have have its own position vector\n",
        "        self.lm_head = nn.Linear(NUM_EMBEDDING_DIMS, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers (B- Batch size, T- Time/ContextLength/BLOCK_SIZE)\n",
        "        token_embeddings = self.token_embedding_table(idx)        ## (B, T, C) -- (C - Number of channels, i.e. NUM_EMBEDDING_DIMS).\n",
        "        pos_embeddings = self.position_embedding_table(torch.arange(T, device=DEVICE))           ## (T, C) - Embeddings for integers from 0 to T-1. All these get embeded through the table.\n",
        "        x = token_embeddings + pos_embeddings\n",
        "        logits = self.lm_head(x)                     ## (B, T, vocab_size). Logits - scores for the next character in the sequence.\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)                  ## 3  Dimnesional --> 2 Dimnesional. Multiplying B and T will stretch all the characters (B * T) into a single row while maintaining Channel dimension\n",
        "            targets = targets.view(B * T)                   ## 2 Dimensional --> 1 Dimensional. Multiplying B and T will stretch all the characters (B * T) into a single row.\n",
        "            loss = F.cross_entropy(logits, targets)         ## cross_entropy --> Negative log-lieklihood = -log(probability(idx)) - Base Loss = -log(1/vocab_size)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\" generate will add characters one by one at time step t+1 upto t + max_new_tokens (in time dimension) for every batch.\n",
        "        idx array gets updated be concatenating every new character prediction to idx \"\"\"\n",
        "\n",
        "        ## idx (B, T) - array of indices of some characters in the current context in a batch.\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # Get predictions from model\n",
        "            logtis, loss = self(idx)                                        ## Calling the forward() function\n",
        "\n",
        "            # Focussing on only the last time-step\n",
        "            logtis = logtis[:, -1, :]       ## (B, T, C) becomes (B, C). Plucking out the the last element in the time dimension, as that is the prediction for what comes next\n",
        "\n",
        "            # Applying softmax to get probabilities\n",
        "            probs = F.softmax(logtis, dim=-1)       ## (B, C)\n",
        "\n",
        "            # Sampling from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)      ## (B, 1) - Picking the next character (one - num_samples) index that has the highest probability. One new character per batch.\n",
        "\n",
        "            # Appending sampled index to the running sequence.\n",
        "            idx = torch.cat((idx, idx_next), dim=1)             ## (B, T + 1). Concatenating the idx of the generated character (idx_next) at time dimension (dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "aAV_QNSC-VHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)           ## Actual Models usually lr=3e-4"
      ],
      "metadata": {
        "id": "iE5aDzwZf2lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = model(xb, yb)\n",
        "print(f\"Logtis Shape: {logits.shape}\")\n",
        "print(f\"{loss=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_tAWs6DFKZL",
        "outputId": "02dde9d0-a944-4df9-c885-74f7723c53b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logtis Shape: torch.Size([256, 65])\n",
            "loss=tensor(4.4157, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)                 ## (1, 1) - one batch, context length = 1\n",
        "print(decode(model.generate(context, max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "id": "heDlK65DFmsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2e1215-d399-4305-bc28-0ffd4c6431b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "?qf;xbDkRZkNdc'wf,ZTkOLOn,eCtK\n",
            "\n",
            "HqPjCkMBbAAU!:XaSvgO-33jMBF?gaTauhXFYVXJtpXeNuwqcBCxv.t?aF dXl!DZaAeWFwccHwyRWf,fDEZaYzxzrEom:\n",
            "Yo3&$FmtofCiaIvB!!BV!$W;nd!lNxc\n",
            "e3 ixYe-EYnkciK;lSq;HFtEZkoG EtSXMB;qWklG.YGZW.FeWjbm!pelJljnFAUVQv.t-hxD3qcdcpvDN:?SuO;MOie'XVUwty.OJlvBPUHI.cBm&pjY-lgvIEjVk:D:lqwJdlGMtS!klGoRW-SQAFQPdGCeIib3qI'TStC&lE$HZLETxgeF3QJ$FsLp-LB3:Ar-xT3H\n",
            "\n",
            "epkO\n",
            "mnvnrufW!A '\n",
            ";;3;QDLWwm:f'E,Cey$f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ITERS = 10000\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "    # Every once in a while evaluate the loss on train and val sets\n",
        "    if iter % EVAL_INTERVAL == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"STEP {iter}: Train Loss - {losses['train']:.4f} \\t Validation Loss - {losses['val']:.4f}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXupEhV8g_yd",
        "outputId": "c207df2a-b6f3-4316-ea68-6f6319a6e61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 0: Train Loss - 4.3886 \t Validation Loss - 4.3734\n",
            "STEP 300: Train Loss - 2.9031 \t Validation Loss - 2.9028\n",
            "STEP 600: Train Loss - 2.6507 \t Validation Loss - 2.6722\n",
            "STEP 900: Train Loss - 2.5712 \t Validation Loss - 2.5843\n",
            "STEP 1200: Train Loss - 2.5505 \t Validation Loss - 2.5602\n",
            "STEP 1500: Train Loss - 2.5195 \t Validation Loss - 2.5373\n",
            "STEP 1800: Train Loss - 2.5092 \t Validation Loss - 2.5273\n",
            "STEP 2100: Train Loss - 2.5053 \t Validation Loss - 2.5090\n",
            "STEP 2400: Train Loss - 2.4930 \t Validation Loss - 2.5112\n",
            "STEP 2700: Train Loss - 2.5030 \t Validation Loss - 2.5096\n",
            "STEP 3000: Train Loss - 2.4877 \t Validation Loss - 2.5090\n",
            "STEP 3300: Train Loss - 2.4922 \t Validation Loss - 2.4938\n",
            "STEP 3600: Train Loss - 2.4760 \t Validation Loss - 2.5041\n",
            "STEP 3900: Train Loss - 2.4885 \t Validation Loss - 2.5042\n",
            "STEP 4200: Train Loss - 2.4822 \t Validation Loss - 2.4993\n",
            "STEP 4500: Train Loss - 2.4729 \t Validation Loss - 2.4895\n",
            "STEP 4800: Train Loss - 2.4711 \t Validation Loss - 2.4919\n",
            "STEP 5100: Train Loss - 2.4762 \t Validation Loss - 2.4973\n",
            "STEP 5400: Train Loss - 2.4761 \t Validation Loss - 2.4991\n",
            "STEP 5700: Train Loss - 2.4815 \t Validation Loss - 2.4848\n",
            "STEP 6000: Train Loss - 2.4785 \t Validation Loss - 2.4746\n",
            "STEP 6300: Train Loss - 2.4704 \t Validation Loss - 2.4921\n",
            "STEP 6600: Train Loss - 2.4829 \t Validation Loss - 2.4952\n",
            "STEP 6900: Train Loss - 2.4597 \t Validation Loss - 2.4953\n",
            "STEP 7200: Train Loss - 2.4624 \t Validation Loss - 2.4887\n",
            "STEP 7500: Train Loss - 2.4656 \t Validation Loss - 2.4941\n",
            "STEP 7800: Train Loss - 2.4512 \t Validation Loss - 2.4973\n",
            "STEP 8100: Train Loss - 2.4684 \t Validation Loss - 2.4947\n",
            "STEP 8400: Train Loss - 2.4632 \t Validation Loss - 2.4895\n",
            "STEP 8700: Train Loss - 2.4665 \t Validation Loss - 2.4826\n",
            "STEP 9000: Train Loss - 2.4592 \t Validation Loss - 2.4859\n",
            "STEP 9300: Train Loss - 2.4622 \t Validation Loss - 2.4911\n",
            "STEP 9600: Train Loss - 2.4694 \t Validation Loss - 2.4934\n",
            "STEP 9900: Train Loss - 2.4605 \t Validation Loss - 2.4875\n",
            "2.422550678253174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)                 ## (1, 1) - one batch, context length = 1\n",
        "print(decode(model.generate(context, max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeXLQYJIjXYN",
        "outputId": "7021133f-bc37-49cd-df66-46b277043027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MPUESTh, af Pre?\n",
            "\n",
            "WISo myouryoube!\n",
            "KENoby ak\n",
            "Sadsal thes ghesthidin cour ay aney Iry ts I fr t ce.\n",
            "Jonghe nd, bemary.\n",
            "Yor 'sour mend sora an hy t--pond betwe ten.\n",
            "Sand thoware in s th llety od, wiourco ffepyotssththas l.\n",
            "TAn.\n",
            "Mourethal intherse ed Ped he movetour?\n",
            "Cassce oros cok hedin tie s ind aus te fe f tas ny, ct CINovecest hes, n id, I fo, mo mane.\n",
            "\n",
            "Anthataker aghercobun ws m k s withoumas F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention"
      ],
      "metadata": {
        "id": "ELzn3EfApyRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information aggregation using average for every individual batch\n",
        "# Averaging the weights (embeddings of each token) upto time_step t of context for every time_step will aggregrate all the imformation upto time_step t\n",
        "# This is very in efficient, lossy form of aggregation\n",
        "# This information can only be done upto the current time-step past tokens cannot know what the future holds\n",
        "# This is basically a weighted aggregration"
      ],
      "metadata": {
        "id": "aGXnQRWsr4rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1"
      ],
      "metadata": {
        "id": "fisHU-QDZtDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, C = 4, 8, 2               ## B - BATCH SIZE, T - CONTEXT LENGTH (TIME DIMENSION), C - CHANNEL SIZE (EMBEDDING SIZE)\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwbfv18h1NUZ",
        "outputId": "e23e68b5-6877-4cef-eb7d-278e66f3a30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b, t] = mean_{i <= t} for x[b, i]\n",
        "\n",
        "xbow = torch.zeros((B, T, C))               ## BOW - Bag of words - said when putting averages together.\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]                   ## (t, C). We want to take the average of the embeddings upto the current time step\n",
        "        xbow[b, t] = torch.mean(xprev, dim=0)"
      ],
      "metadata": {
        "id": "nX6UWV1D1g_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E_qCagj2ZUn",
        "outputId": "624db9ea-68e4-43a8-9cb7-61428f8c10fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0939, -1.0366],\n",
              "        [ 2.3651, -1.1340],\n",
              "        [ 0.1826,  0.4906],\n",
              "        [ 0.4017, -0.3161],\n",
              "        [-0.7491, -0.7988],\n",
              "        [-1.3352, -1.3028],\n",
              "        [ 0.0301,  1.5415],\n",
              "        [ 0.4123, -0.1268]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nebps1gv2caT",
        "outputId": "13e4f999-b58a-4a0c-af0b-1408e13d18e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0939, -1.0366],\n",
              "        [ 1.2295, -1.0853],\n",
              "        [ 0.8805, -0.5600],\n",
              "        [ 0.7608, -0.4990],\n",
              "        [ 0.4588, -0.5590],\n",
              "        [ 0.1598, -0.6830],\n",
              "        [ 0.1413, -0.3652],\n",
              "        [ 0.1752, -0.3354]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2"
      ],
      "metadata": {
        "id": "EQPw09QYZ2Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The aggregration process above can easily be achieved using matrix multiplication\n",
        "# Using the lower triangle matrix [1\\0] - ones in lower half of the traingle, zreos in the upper half\n",
        "# Multiplying the embeddings vector with the lower triangle will set the information for all the tokens after the current token to be zero\n",
        "# This will allow us to agregrate embeddings upto the current timestep in the context.\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.ones(3, 3)\n",
        "a = torch.tril(a)\n",
        "a = a / torch.sum(a, dim=1, keepdims=True)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b                                                           ## Aggregration through matrix multiplication\n",
        "\n",
        "print(f\"a = \\n{a}\\n\\n{'-'*30}\\n\")\n",
        "print(f\"b = \\n{b}\\n\\n{'-'*30}\\n\")\n",
        "print(f\"c = \\n{c}\\n\\n{'-'*30}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXCzABXo4I6G",
        "outputId": "a0f72534-5183-47b9-f68f-bbc45f6a1e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = \n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "\n",
            "------------------------------\n",
            "\n",
            "b = \n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "\n",
            "------------------------------\n",
            "\n",
            "c = \n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight = torch.tril(torch.ones(T, T))\n",
        "weight = weight / weight.sum(dim=1, keepdims=True)              ## Normalizing\n",
        "xbow2 = weight @ x                                              ## Aggregration through matrix multiplication\n",
        "                                                                ## weight (T, T) @ x (B, T, C): Batched matrix multiplication ----> Here \"B\" gets broadcasted as (B, T, T) @ (B, T, C) ==> (B, T, C)\n",
        "xbow2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvVge7BKZ9O0",
        "outputId": "5bd77b07-c92a-42dd-de35-aa732a66d701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(xbow, xbow2)         ## XBOW === XBOW2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFGMJv3hcOXT",
        "outputId": "d506dd69-c060-4f5e-9f1c-55bb8cf04147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0], xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj5l9M4qcXv-",
        "outputId": "aa4b19a4-2a42-454b-8f8f-56974a03c6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0939, -1.0366],\n",
              "         [ 1.2295, -1.0853],\n",
              "         [ 0.8805, -0.5600],\n",
              "         [ 0.7608, -0.4990],\n",
              "         [ 0.4588, -0.5590],\n",
              "         [ 0.1598, -0.6830],\n",
              "         [ 0.1413, -0.3652],\n",
              "         [ 0.1752, -0.3354]]),\n",
              " tensor([[ 0.0939, -1.0366],\n",
              "         [ 1.2295, -1.0853],\n",
              "         [ 0.8805, -0.5600],\n",
              "         [ 0.7608, -0.4990],\n",
              "         [ 0.4588, -0.5590],\n",
              "         [ 0.1598, -0.6830],\n",
              "         [ 0.1413, -0.3652],\n",
              "         [ 0.1752, -0.3354]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 3\n",
        "- Using SoftMax()\n",
        "- We use softmax in self attention because, the weights (i.e. the interaction strenght or token affinity) begin with zero.\n",
        "- These weights form the lower⛛ matrix tell us how much of each token from the past do we want to aggregrate and average up.\n",
        "- By setting 0 from lower⛛ matrix to \"-inf\", we are saying that tokens token from the past cannot communicate with the tokens from the future. No information will be aggregrated from locations where the value is -inf.\n",
        "- The affinities between tokens are going to be data dependent and not be set to zero constantly. And some tokens are going to find some other tokens more or less interesting depending on their values."
      ],
      "metadata": {
        "id": "gpzLf-rlZ7MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "weight = torch.zeros((T, T))\n",
        "weight = weight.masked_fill(tril==0, float('-inf'))         ## In tril replace all 0 with -inf\n",
        "weight = F.softmax(weight, dim=1)                           ## softmax - normalizing function. exp(0)=1 and exp(inf)=0\n",
        "xbow3 = weight @ x                                          ## Aggregration through matrix multiplication"
      ],
      "metadata": {
        "id": "Tlz6fSgMZ92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(xbow, xbow3)                             ## xbow == xbow3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8obC2XGesZC",
        "outputId": "3a8d6bba-3dca-4031-e6e8-20684f2ba88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0], xbow3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrotn-vZezqU",
        "outputId": "e8c91e14-e6f3-4950-9000-70a4a6a0126d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0939, -1.0366],\n",
              "         [ 1.2295, -1.0853],\n",
              "         [ 0.8805, -0.5600],\n",
              "         [ 0.7608, -0.4990],\n",
              "         [ 0.4588, -0.5590],\n",
              "         [ 0.1598, -0.6830],\n",
              "         [ 0.1413, -0.3652],\n",
              "         [ 0.1752, -0.3354]]),\n",
              " tensor([[ 0.0939, -1.0366],\n",
              "         [ 1.2295, -1.0853],\n",
              "         [ 0.8805, -0.5600],\n",
              "         [ 0.7608, -0.4990],\n",
              "         [ 0.4588, -0.5590],\n",
              "         [ 0.1598, -0.6830],\n",
              "         [ 0.1413, -0.3652],\n",
              "         [ 0.1752, -0.3354]]))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 4: Self Attention\n",
        "- Self attention implementation for a single head."
      ],
      "metadata": {
        "id": "nhlCeWAXAydR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://www.youtube.com/watch?v=U0s0f995w14&ab_channel=AladdinPersson\n",
        "- https://www.youtube.com/watch?v=pkVwUVEHmfI&ab_channel=AladdinPersson\n",
        "- https://peterbloem.nl/blog/transformers\n",
        "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
      ],
      "metadata": {
        "id": "YczcI7frmafR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot Product Attention: Attention(Q, K, V) = softmax(Q * K^t / sqrt(dk)) * V\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, num_heads, EMBEDDING_DIMS):\n",
        "        pass"
      ],
      "metadata": {
        "id": "rBLHi1FDA9dT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}